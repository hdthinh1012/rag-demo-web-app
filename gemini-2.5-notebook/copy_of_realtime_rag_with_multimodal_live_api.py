# -*- coding: utf-8 -*-
"""Copy of Realtime RAG with Multimodal Live API.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LenTAOguLBl8CnlnXL-UB_O7TOxjCEbo
"""

# Copyright 2024 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""# Real-time Retrieval Augmented Generation (RAG) using the Vertex AI Multimodal Live API

<table align="left">
  <td style="text-align: center">
    <a href="https://colab.research.google.com/github/GoogleCloudPlatform/generative-ai/blob/main/gemini/multimodal-live-api/real_time_rag_retail_gemini_2_0.ipynb">
      <img width="32px" src="https://www.gstatic.com/pantheon/images/bigquery/welcome_page/colab-logo.svg" alt="Google Colaboratory logo"><br> Open in Colab
    </a>
  </td>
  <td style="text-align: center">
    <a href="https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fgenerative-ai%2Fmain%2Fgemini%2Fmultimodal-live-api%2Freal_time_rag_retail_gemini_2_0.ipynb">
      <img width="32px" src="https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN" alt="Google Cloud Colab Enterprise logo"><br> Open in Colab Enterprise
    </a>
  </td>
  <td style="text-align: center">
    <a href="https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/gemini/multimodal-live-api/real_time_rag_retail_gemini_2_0.ipynb">
      <img src="https://www.gstatic.com/images/branding/gcpiconscolors/vertexai/v1/32px.svg" alt="Vertex AI logo"><br> Open in Vertex AI Workbench
    </a>
  </td>
  <td style="text-align: center">
    <a href="https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/multimodal-live-api/real_time_rag_retail_gemini_2_0.ipynb">
      <img width="32px" src="https://www.svgrepo.com/download/217753/github.svg" alt="GitHub logo"><br> View on GitHub
    </a>
  </td>
</table>

<br/>

<br/>

<div style="clear: both;"></div>
<br/>
<b>Share to:</b>

<a href="https://www.linkedin.com/sharing/share-offsite/?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/multimodal-live-api/real_time_rag_retail_gemini_2_0.ipynb" target="_blank">
  <img width="20px" src="https://upload.wikimedia.org/wikipedia/commons/8/81/LinkedIn_icon.svg" alt="LinkedIn logo">
</a>

<a href="https://bsky.app/intent/compose?text=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/multimodal-live-api/real_time_rag_retail_gemini_2_0.ipynb" target="_blank">
  <img width="20px" src="https://upload.wikimedia.org/wikipedia/commons/7/7a/Bluesky_Logo.svg" alt="Bluesky logo">
</a>

<a href="https://twitter.com/intent/tweet?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/multimodal-live-api/real_time_rag_retail_gemini_2_0.ipynb" target="_blank">
  <img width="20px" src="https://upload.wikimedia.org/wikipedia/commons/5/5a/X_icon_2.svg" alt="X logo">
</a>

<a href="https://reddit.com/submit?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/multimodal-live-api/real_time_rag_retail_gemini_2_0.ipynb" target="_blank">
  <img width="20px" src="https://redditinc.com/hubfs/Reddit%20Inc/Brand/Reddit_Logo.png" alt="Reddit logo">
</a>

<a href="https://www.facebook.com/sharer/sharer.php?u=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/multimodal-live-api/real_time_rag_retail_gemini_2_0.ipynb" target="_blank">
  <img width="20px" src="https://upload.wikimedia.org/wikipedia/commons/5/51/Facebook_f_logo_%282019%29.svg" alt="Facebook logo">
</a>

| | |
|-|-|
| Author(s) | [Deepak Moonat](https://github.com/dmoonat/) |

<div class="alert alert-block alert-warning">
<b>
‚ö†Ô∏è Gemini 2.0 Flash (Model ID: <code>gemini-2.0-flash-live-preview-04-09</code>) and the Google Gen AI SDK are currently experimental and output can vary ‚ö†Ô∏è</b>
</div>

## Overview

This notebook provides a comprehensive demonstration of the Vertex AI Gemini and Multimodal Live APIs, showcasing text and audio generation capabilities.  Users will learn to develop a real-time Retrieval Augmented Generation (RAG) system leveraging the Multimodal Live API for a retail use-case. This system will generate audio and text responses grounded in provided documents. The tutorial covers the following:

- **Gemini API:** Text output generation.
- **Multimodal Live API:** Text and audio output generation.
- **Retrieval Augmented Generation (RAG):**  Text and audio output generation grounded in provided documents for a retail use-case.

## Get started

### Gemini 2.5

Gemini 2.5 Flash with Live API native audio features our cutting-edge native audio functionality for Live API. In addition to the standard Live API features, this preview model includes:

Enhanced voice quality and adaptability: Live API native audio provides richer, more natural voice interactions with 30 HD voices in 24 languages.
Introducing Proactive Audio: When Proactive Audio is enabled, the model only responds when it's relevant. The model generates text transcripts and audio responses proactively only for queries directed to the device, and does not respond to non-device directed queries.
Introducing Affective Dialog: Models using Live API native audio can understand and respond appropriately to users' emotional expressions for more nuanced conversations.

### Gemini 2.0

[Gemini 2.0 Flash](https://cloud.google.com/vertex-ai/generative-ai/docs/gemini-v2) is a new multimodal generative ai model from the Gemini family developed by [Google DeepMind](https://deepmind.google/). It now available as an experimental preview release through the Gemini API in Vertex AI and Vertex AI Studio. The model introduces new features and enhanced core capabilities:

- Multimodal Live API: This new API helps you create real-time vision and audio streaming applications with tool use.
- Speed and performance: Gemini 2.0 Flash is the fastest model in the industry, with a 3x improvement in time to first token (TTFT) over 1.5 Flash.
- Quality: The model maintains quality comparable to larger models like Gemini 2.0 and GPT-4o.
- Improved agentic experiences: Gemini 2.0 delivers improvements to multimodal understanding, coding, complex instruction following, and function calling.
- New Modalities: Gemini 2.0 introduces native image generation and controllable text-to-speech capabilities, enabling image editing, localized artwork creation, and expressive storytelling.
- To support the new model, we're also shipping an all new SDK that supports simple migration between the Gemini Developer API and the Gemini API in Vertex AI.

### Install Dependencies

- `google-genai`:  Google Gen AI python library
- `PyPDF2`: To read PDFs
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# 
# %pip install --upgrade --quiet google-genai PyPDF2

"""### Restart runtime

To use the newly installed packages in this Jupyter runtime, you must restart the runtime. You can do this by running the cell below, which restarts the current kernel.

The restart might take a minute or longer. After it's restarted, continue to the next step.
"""

import IPython

app = IPython.Application.instance()
app.kernel.do_shutdown(True)

"""<div class="alert alert-block alert-warning">
<b>‚ö†Ô∏è The kernel is going to restart. Wait until it's finished before continuing to the next step. ‚ö†Ô∏è</b>
</div>

### Authenticate your notebook environment (Colab only)

If you're running this notebook on Google Colab, run the cell below to authenticate your environment.
"""

import sys

if "google.colab" in sys.modules:
    from google.colab import auth

    auth.authenticate_user()

"""### Set Google Cloud project information

To get started using Vertex AI, you must have an existing Google Cloud project and [enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com).

Learn more about [setting up a project and a development environment](https://cloud.google.com/vertex-ai/docs/start/cloud-environment).
"""

import os

PROJECT_ID = "supple-kayak-466408-a3"

LOCATION = "us-central1"

"""### Import libraries"""

# For asynchronous operations
import asyncio

# For data processing
import glob
from typing import Any

from IPython.display import Audio, Markdown, display
import PyPDF2

# For GenerativeAI
from google import genai
from google.genai import types
from google.genai.types import LiveConnectConfig
import numpy as np
import pandas as pd

# For similarity score
from sklearn.metrics.pairwise import cosine_similarity

# For retry mechanism
from tenacity import retry, stop_after_attempt, wait_random_exponential

"""#### Initialize Gen AI client

- Client for calling the Gemini API in Vertex AI
- `vertexai=True`, indicates the client should communicate with the Vertex AI API endpoints.
"""

# Vertex AI API
client = genai.Client(
    vertexai=True,
    project=PROJECT_ID,
    location=LOCATION,
)

"""### Initialize model"""

MODEL_ID = "gemini-live-2.5-flash-preview-native-audio"
MODEL = f"projects/{PROJECT_ID}/locations/{LOCATION}/publishers/google/models/{MODEL_ID}"

text_embedding_model = "gemini-embedding-001"

"""## Sample Use Case - Retail Customer Support Assistance

Let's imagine a bicycle shop called `Cymbal Bikes` that offers various services like brake repair, chain replacement, and more. Our goal is to create a straightforward support system that can answer customer questions based on the shop's policies and service offerings.

Having a customer support assistance offers numerous advantages for businesses, ultimately leading to improved customer satisfaction and loyalty, as well as increased profitability. Here are some key benefits:

- Faster Resolution of Issues: Users can quickly find answers to their questions without having to search through store's website.
- Improved Efficiency: The assistant can handle simple, repetitive questions, freeing up human agents to focus on more complex or strategic tasks.
- 24/7 Availability: Unlike human colleagues, the assistant is available around the clock, providing immediate support regardless of time zones or working hours.
- Consistent Information: The assistant provides standardized answers, ensuring consistency and accuracy.

#### Context Documents

- Download the documents from Google Cloud Storage bucket
- These documents are specific to `Cymbal Bikes` store
  - [`Cymbal Bikes Return Policy`](https://storage.googleapis.com/github-repo/generative-ai/gemini2/use-cases/retail_rag/documents/CymbalBikesReturnPolicy.pdf): Contains information about return policy
  - [`Cymbal Bikes Services`](https://storage.googleapis.com/github-repo/generative-ai/gemini2/use-cases/retail_rag/documents/CymbalBikesServices.pdf): Contains information about services provided by Cymbal Bikes
"""

!gsutil cp "gs://github-repo/generative-ai/gemini2/use-cases/retail_rag/documents/CymbalBikesReturnPolicy.pdf" "documents/CymbalBikesReturnPolicy.pdf"
!gsutil cp "gs://github-repo/generative-ai/gemini2/use-cases/retail_rag/documents/CymbalBikesServices.pdf" "documents/CymbalBikesServices.pdf"
!curl -O "https://arxiv.org/pdf/2507.06261v1" "documents/GeminiReport.pdf"

"""### Text

- Let's check a specific query to our retail use-case
"""

query = "What is the price of a basic tune-up at Cymbal Bikes?"

response = client.models.generate_content(
    model="gemini-2.5-flash",
    contents=query,
)

display(Markdown(response.text))

query = "List of models in Gemini 2.5 family"

response = client.models.generate_content(
    model="gemini-2.5-flash",
    contents=query,
)

display(Markdown(response.text))

"""## Multimodal Live API

The multimodal live API enables you to build low-latency, multi-modal applications. It currently supports text as input and text & audio as output.

- Low Latency, where audio output is required, where the Text-to-Speech step can be skipped
- Provides a more interactive user experience.
- Suitable for applications requiring immediate audio feedback

See the [Multimodal Live API](https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/multimodal-live) page for more details.

#### Asynchronous (async) operation

When to use async calls:
1. **I/O-bound operations**: When your code spends a significant amount of time waiting for external resources
   (e.g., network requests, file operations, database queries). Async allows other tasks to run while waiting.  
   This is especially beneficial for real-time applications or when dealing with multiple concurrent requests.
   
   Example:
   - Fetching data from a remote server.

2. **Parallel tasks**: When you have independent tasks that can run concurrently without blocking each other. Async
   allows you to efficiently utilize multiple CPU cores or network connections.
   
   Example:
   - Processing a large number of prompts and generating audio for each.


3. **User interfaces**: In applications with graphical user interfaces (GUIs), async operations prevent the UI from
   freezing while performing long-running tasks. Users can interact with the interface even when background
   operations are active.
   
   Example:  
   - A chatbot interacting in real time, where an audio response is generated in the background.

### Text

For text generation, you need to set the `response_modalities` to `TEXT`
"""

async def generate_content(query: str) -> str:
    """Function to generate text content using Gemini live API.

    Args:
      query: The query to generate content for.

    Returns:
      The generated content.
    """
    config = LiveConnectConfig(response_modalities=["TEXT"])

    async with client.aio.live.connect(model=MODEL, config=config) as session:

        await session.send(input=query, end_of_turn=True)

        response = []
        async for message in session.receive():
            try:
                if message.text:
                    response.append(message.text)
            except AttributeError:
                pass

            if message.server_content.turn_complete:
                response = "".join(str(x) for x in response)
                return response

"""- Try a specific query"""

query = "What is the price of a basic tune-up at Cymbal Bikes?"

response = await generate_content(query)
display(Markdown(response))

"""### Audio

- For audio generation, you need to set the `response_modalities` to `AUDIO`
"""

async def generate_audio_content(query: str):
    """Function to generate audio response for provided query using Gemini Multimodal Live API.

    Args:
      query: The query to generate audio response for.

    Returns:
      The audio response.
    """
    config = {
        "response_modalities": ["AUDIO"],
        "speech_config": {
            "voice_config": {"prebuilt_voice_config": {"voice_name": "Kore"}},
            "language_code": "vi-VI"
        },
    }

    async with client.aio.live.connect(model=MODEL, config=config) as session:

        await session.send(input=query, end_of_turn=True)

        audio_parts = []
        async for message in session.receive():
            if message.server_content.model_turn:
                for part in message.server_content.model_turn.parts:
                    if part.inline_data:
                        audio_parts.append(
                            np.frombuffer(part.inline_data.data, dtype=np.int16)
                        )

            if message.server_content.turn_complete:
                if audio_parts:
                    audio_data = np.concatenate(audio_parts, axis=0)
                    await asyncio.sleep(0.4)
                    display(Audio(audio_data, rate=24000, autoplay=True))
                break

"""In this example, you send a text prompt and request the model response in audio.

- Let's check the same query as before
"""

# query = "What is the price of a basic tune-up at Cymbal Bikes?, Response in Vietnamese"
query = "What is the newest version of Gemini?, Response in Vietnamese"

await generate_audio_content(query)

"""- Model is unable to answer the query, but with the Multimodal Live API, it doesn't hallucinate, which is pretty good!!

### Continuous Audio Interaction (Not multiturn)

- Below function generates audio output based on the provided text prompt.
  - The generated audio is displayed using `IPython.display.Audio`.

- Input your prompts (type `q` or `quit` or `exit` to exit).
- Example prompts:
  - Hello
  - Who are you?
  - What's the largest planet in our solar system?
  - Tell me 3 fun facts about the universe?
"""

async def continuous_audio_generation():
    """Continuously generates audio responses for the asked queries."""
    while True:
        query = input("Your query > ")
        if any(query.lower() in s for s in ["q", "quit", "exit"]):
            break
        await generate_audio_content(query)


await continuous_audio_generation()

"""## Enhancing LLM Accuracy with RAG

We'll be showcasing the design pattern for how to implement Real-time Retrieval Augmented Generation (RAG) using Gemini 2.0 multimodal live API.

- Multimodal live API uses websockets to communicate over the internet
- It maintains a continuous connection
- Ideal for real-time applications which require persistent communication


> Note: Replicating real-life scenarios with Python can be challenging within the constraints of a Colab environment.


However, the flow shown in this section can be modified for streaming audio input and output.

<br/>

We'll build the RAG pipeline from scratch to help you understand each and every components of the pipeline.

There are other ways to build the RAG pipeline using open source tools such as [LangChain](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/retrieval-augmented-generation/multimodal_rag_langchain.ipynb), [LlamaIndex](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/retrieval-augmented-generation/llamaindex_rag.ipynb) etc.

### Context Documents

- Documents are the building blocks of any RAG pipeline, as it provides the relevant context needed to ground the LLM responses
- We'll be using the documents already downloaded at the start of the notebook
"""

import glob
documents = glob.glob("documents/*")
documents

"""### Retrieval Augmented Generation Architecture

In general, RAG architecture consists of the following components

**Data Preparation**
1. Chunking: Dividing the document into smaller, manageable pieces for processing.
2. Embedding: Transforming text chunks into numerical vectors representing semantic meaning.
3. Indexing: Organizing embeddings for efficient similarity search.

![RAGArchitecture](https://storage.googleapis.com/github-repo/generative-ai/gemini2/use-cases/retail_rag/images/RAGArchitecture.png)

**Inference**
1. Retrieval: Finding the most relevant chunks based on the query embedding.
2. Query Augmentation: Enhancing the query with retrieved context for improved generation.
3. Generation: Synthesizing a coherent and informative answer based on the augmented query.

![LiveAPI](https://storage.googleapis.com/github-repo/generative-ai/gemini2/use-cases/retail_rag/images/LiveAPI.png)

#### Document Embedding and Indexing

Following blocks of code shows how to process unstructured data(PDFs), extract text, and divide them into smaller chunks for efficient embedding and retrieval.

- Embeddings:
  - Numerical representations of text
  - It capture the semantic meaning and context of the text
  - We'll use Vertex AI's text embedding model to generate embeddings
  - Error handling (like the retry mechanism) during embedding generation due to potential API quota limits.

- Indexing:
  - Build a searchable index from embeddings, enabling efficient similarity search.
  - For example, the index is like a detailed table of contents for a massive reference book.


Check out the Google Cloud Platform [documentation](https://cloud.google.com/vertex-ai/generative-ai/docs/embeddings) for detailed understanding and example use-cases.
"""

@retry(wait=wait_random_exponential(multiplier=1, max=120), stop=stop_after_attempt(4))
def get_embeddings(
    embedding_client: Any, embedding_model: str, text: str, output_dim: int = 768
) -> list[float]:
    """
    Generate embeddings for text with retry logic for API quota management.

    Args:
        embedding_client: The client object used to generate embeddings.
        embedding_model: The name of the embedding model to use.
        text: The text for which to generate embeddings.
        output_dim: The desired dimensionality of the output embeddings (default is 768).

    Returns:
        A list of floats representing the generated embeddings. Returns None if a "RESOURCE_EXHAUSTED" error occurs.

    Raises:
        Exception: Any exception encountered during embedding generation, excluding "RESOURCE_EXHAUSTED" errors.
    """
    try:
        response = embedding_client.models.embed_content(
            model=embedding_model,
            contents=[text],
            config=types.EmbedContentConfig(output_dimensionality=output_dim),
        )
        return [response.embeddings[0].values]
    except Exception as e:
        if "RESOURCE_EXHAUSTED" in str(e):
            return None
        print(f"Error generating embeddings: {str(e)}")
        raise

"""- The code block executes the following steps:

  - Extracts text from PDF documents and segments it into smaller chunks for processing.
  - Employs a Vertex AI model to transform each text chunk into a numerical embedding vector, facilitating semantic representation and search.
  - Constructs a Pandas DataFrame to store the embeddings, enriched with metadata such as document name and page number, effectively creating a searchable index for efficient retrieval.

"""

def build_index(
    document_paths: list[str],
    embedding_client: Any,
    embedding_model: str,
    chunk_size: int = 512,
) -> pd.DataFrame:
    """
    Build searchable index from a list of PDF documents with page-wise processing.

    Args:
        document_paths: A list of file paths to PDF documents.
        embedding_client: The client object used to generate embeddings.
        embedding_model: The name of the embedding model to use.
        chunk_size: The maximum size (in characters) of each text chunk.  Defaults to 512.

    Returns:
        A Pandas DataFrame where each row represents a text chunk.  The DataFrame includes columns for:
            - 'document_name': The path to the source PDF document.
            - 'page_number': The page number within the document.
            - 'page_text': The full text of the page.
            - 'chunk_number': The chunk number within the page.
            - 'chunk_text': The text content of the chunk.
            - 'embeddings': The embedding vector for the chunk.

    Raises:
        ValueError: If no chunks are created from the input documents.
        Exception:  Any exceptions encountered during file processing are printed to the console and the function continues to the next document.
    """
    all_chunks = []

    for doc_path in document_paths:
        try:
            with open(doc_path, "rb") as file:
                pdf_reader = PyPDF2.PdfReader(file)

                for page_num in range(len(pdf_reader.pages)):
                    page = pdf_reader.pages[page_num]
                    page_text = page.extract_text()

                    chunks = [
                        page_text[i : i + chunk_size]
                        for i in range(0, len(page_text), chunk_size)
                    ]

                    for chunk_num, chunk_text in enumerate(chunks):
                        embeddings = get_embeddings(
                            embedding_client, embedding_model, chunk_text
                        )

                        if embeddings is None:
                            print(
                                f"Warning: Could not generate embeddings for chunk {chunk_num} on page {page_num + 1}"
                            )
                            continue

                        chunk_info = {
                            "document_name": doc_path,
                            "page_number": page_num + 1,
                            "page_text": page_text,
                            "chunk_number": chunk_num,
                            "chunk_text": chunk_text,
                            "embeddings": embeddings,
                        }
                        all_chunks.append(chunk_info)

        except Exception as e:
            print(f"Error processing document {doc_path}: {str(e)}")
            continue

    if not all_chunks:
        raise ValueError("No chunks were created from the documents")

    return pd.DataFrame(all_chunks)

"""Let's create embeddings and an index using the provided documents"""

vector_db_mini_vertex = build_index(
    documents, embedding_client=client, embedding_model=text_embedding_model
)
vector_db_mini_vertex

# Index size
len(vector_db_mini_vertex["embeddings"][0][0])

# Example of how a chunk looks like
vector_db_mini_vertex.loc[0, "chunk_text"]

"""To enhance the performance of retrieval systems, consider the following:

- Optimize chunk size selection to balance granularity and context.
- Evaluate various chunking strategies to identify the most effective approach for your data.
- Explore managed services and scalable indexing solutions, such as [Vertex AI Search](https://cloud.google.com/generative-ai-app-builder/docs/create-datastore-ingest), to enhance performance and efficiency.

#### Retrieval

The below code demonstrates how to query the index and uses a cosine similarity measure for comparing query vectors against the index.

* **Input:** Accepts a query string and parameters like the number of relevant chunks to return.
* **Embedding Generation:** Generates an embedding for the input query using the same model used to embed the document chunks.
* **Similarity Search:** Compares the query embedding to the embeddings of all indexed document chunks, using cosine similarity.  Could use other distance metrics as well.
* **Ranking:** Ranks the chunks based on their similarity scores to the query.
* **Top-k Retrieval:** Returns the top *k* most similar chunks, where *k* is specified by the input parameters.  This could be configurable.
* **Output:** Returns a list of relevant chunks, potentially including the original chunk text, similarity score, document source (filename, page number), and chunk metadata.
"""

def get_relevant_chunks(
    query: str,
    vector_db: pd.DataFrame,
    embedding_client: Any,
    embedding_model: str,
    top_k: int = 3,
) -> str:
    """
    Retrieve the most relevant document chunks for a query using similarity search.

    Args:
        query: The search query string.
        vector_db: A pandas DataFrame containing the vectorized document chunks.
                     It must contain columns named 'embeddings', 'document_name',
                     'page_number', and 'chunk_text'.
                     The 'embeddings' column should contain lists or numpy arrays
                     representing the embeddings.
        embedding_client: The client object used to generate embeddings.
        embedding_model: The name of the embedding model to use.
        top_k: The number of most similar chunks to retrieve. Defaults to 3.

    Returns:
        A formatted string containing the top_k most relevant chunks.  Each chunk is
        presented with its page number and chunk number. Returns an error message if
        the query processing fails or if an error occurs during chunk retrieval.

    Raises:
        Exception: If any error occurs during the process (e.g., issues with the embedding client,
                   data format problems in the vector database).
                   The specific error is printed to the console.
    """
    try:
        query_embedding = get_embeddings(embedding_client, embedding_model, query)

        if query_embedding is None:
            return "Could not process query due to quota issues"

        similarities = [
            cosine_similarity(query_embedding, chunk_emb)[0][0]
            for chunk_emb in vector_db["embeddings"]
        ]

        top_indices = np.argsort(similarities)[-top_k:]
        relevant_chunks = vector_db.iloc[top_indices]

        context = []
        for _, row in relevant_chunks.iterrows():
            context.append(
                {
                    "document_name": row["document_name"],
                    "page_number": row["page_number"],
                    "chunk_number": row["chunk_number"],
                    "chunk_text": row["chunk_text"],
                }
            )

        return "\n\n".join(
            [
                f"[Page {chunk['page_number']}, Chunk {chunk['chunk_number']}]: {chunk['chunk_text']}"
                for chunk in context
            ]
        )

    except Exception as e:
        print(f"Error getting relevant chunks: {str(e)}")
        return "Error retrieving relevant chunks"

"""Let's test out our retrieval component

- Let's try the same query for which the model was not able to answer earlier, due to lack of context
"""

query = "What is the newest version of Gemini, list all of different flavor?"
relevant_context = get_relevant_chunks(
    query, vector_db_mini_vertex, client, text_embedding_model, top_k=10
)
relevant_context

"""- You can see, with the help of the relevant context we can derive the answer as it contains the chunks specific to the asked query.

![Context](https://storage.googleapis.com/github-repo/generative-ai/gemini2/use-cases/retail_rag/images/Context.png)

For optimal performance, consider these points:

* **Context Window:**  Considers a context window around the retrieved chunks to provide more comprehensive context.  This could involve returning neighboring chunks or a specified window size.
* **Filtering:** Option to filter retrieved chunks based on criteria like minimum similarity score or source document.
* **Efficiency:** Designed for efficient retrieval, especially for large indexes, potentially using optimized search algorithms or data structures.

### Generation

*   **Contextual Answer Synthesis:** The core function of the generation component is to synthesize a coherent and informative answer based on the retrieved context.  It takes the user's query and the relevant document chunks as input.
*   **Large Language Model (LLM) Integration:**  It leverages a large language model (LLM) to generate the final answer. The LLM processes both the query and the retrieved context to produce a response.  The quality of the answer heavily relies on the capabilities of the chosen LLM.
*   **Coherence and Relevance:**  A good generation function ensures the generated answer is coherent, factually accurate, and directly addresses the user's query, using only the provided context. It avoids hallucinations (generating information not present in the context).
*  **Prompt Engineering:** The effectiveness of the LLM is heavily influenced by the prompt.  The generation function likely incorporates prompt engineering techniques to guide the LLM towards generating the desired output.  This may involve carefully crafting instructions for the LLM or providing examples.

For more details on prompt engineering, check out the [documentation](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/prompt-design-strategies).

Let's see two use-cases, `Text-In-Text-Out` and `Text-In-Audio-Out`
"""

@retry(wait=wait_random_exponential(multiplier=1, max=120), stop=stop_after_attempt(4))
async def generate_answer(
    query: str, context: str, llm_client: Any, modality: str = "text"
) -> str:
    """
    Generate answer using LLM with retry logic for API quota management.

    Args:
        query: User query.
        context: Relevant text providing context for the query.
        llm_client: Client for accessing LLM API.
        modality: Output modality (text or audio).

    Returns:
        Generated answer.

    Raises:
        Exception: If an unexpected error occurs during the LLM call (after retry attempts are exhausted).
    """
    try:
        # If context indicates earlier quota issues, return early
        if context in [
            "Could not process query due to quota issues",
            "Error retrieving relevant chunks",
        ]:
            return "Can't Process, Quota Issues"

        prompt = f"""Based on the following context, please answer the question.

        Context:
        {context}

        Question: {query}

        Answer:"""

        if modality == "text":
            # Generate text answer using LLM
            response = await generate_content(prompt)
            return response

        elif modality == "audio":
            # Generate audio answer using LLM
            await generate_audio_content(prompt)

    except Exception as e:
        if "RESOURCE_EXHAUSTED" in str(e):
            return "Can't Process, Quota Issues"
        print(f"Error generating answer: {str(e)}")
        return "Error generating answer"

"""Let's test our `Generation` component"""

query = "What is the price of a basic tune-up at Cymbal Bikes?"

generated_answer = await generate_answer(
    query, relevant_context, client, modality="text"
)
display(Markdown(generated_answer))

query = "What is the newest version of Gemini, list all of different flavor?"
await generate_answer(query, relevant_context, client, modality="audio")

"""> And the answer is... CORRECT !! üéâ

- The accuracy of the generated answer is attributed to the provision of relevant context to the Large Language Model (LLM), enabling it to effectively comprehend the query and produce an appropriate response.

### Pipeline

Let's put `Retrieval` and `Generation` components together in a pipeline.
"""

async def rag(
    question: str,
    vector_db: pd.DataFrame,
    embedding_client: Any,
    embedding_model: str,
    llm_client: Any,
    top_k: int,
    llm_model: str,
    modality: str = "text",
) -> str | None:
    """
    RAG Pipeline.

    Args:
        question: User query.
        vector_db: DataFrame containing document chunks and embeddings.
        embedding_client: Client for accessing embedding API.
        embedding_model: Name of the embedding model.
        llm_client: Client for accessing LLM API.
        top_k: The number of top relevant chunks to retrieve from the vector database.
        llm_model: Name of the LLM model.
        modality: Output modality (text or audio).

    Returns:
        For text modality, generated answer.
        For audio modality, audio playback widget.

    Raises:
        Exception:  Catches and prints any exceptions during processing. Returns an error message.
    """

    try:
        # Get relevant context for question
        relevant_context = get_relevant_chunks(
            question, vector_db, embedding_client, embedding_model, top_k=top_k
        )

        if modality == "text":
            # Generate text answer using LLM
            generated_answer = await generate_answer(
                question,
                relevant_context,
                llm_client,
            )
            return generated_answer

        elif modality == "audio":
            # Generate audio answer using LLM
            await generate_answer(
                question, relevant_context, llm_client, modality=modality
            )
            return

    except Exception as e:
        print(f"Error processing question '{question}': {str(e)}")
        return {"question": question, "generated_answer": "Error processing question"}

"""Our Retrieval Augmented Generation (RAG) architecture allows for flexible output modality(text and audio) selection. By modifying only the generation component, we can produce both text and audio output while maintaining the same retrieval mechanism. This highlights the adaptability of RAG in catering to diverse content presentation needs.

### Inference

Let's test our simple RAG pipeline

#### Sample Queries
"""

question_set = [
    {
        "question": "What is the price of a basic tune-up at Cymbal Bikes?",
        "answer": "A basic tune-up costs $100.",
    },
    {
        "question": "How much does it cost to replace a tire at Cymbal Bikes?",
        "answer": "Replacing a tire at Cymbal Bikes costs $50 per tire.",
    },
    {
        "question": "What does gear repair at Cymbal Bikes include?",
        "answer": "Gear repair includes inspection and repair of the gears, including replacement of chainrings, cogs, and cables as needed.",
    },
    {
        "question": "What is the cost of replacing a tube at Cymbal Bikes?",
        "answer": "Replacing a tube at Cymbal Bikes costs $20.",
    },
    {
        "question": "Can I return clothing items to Cymbal Bikes?",
        "answer": "Clothing can only be returned if it is unworn and in the original packaging.",
    },
    {
        "question": "What is the time frame for returning items to Cymbal Bikes?",
        "answer": "Cymbal Bikes offers a 30-day return policy on all items.",
    },
    {
        "question": "Can I return edible items like energy gels?",
        "answer": "No, edible items are not returnable.",
    },
    {
        "question": "How can I return an item purchased online from Cymbal Bikes?",
        "answer": "Items purchased online can be returned to any Cymbal Bikes store or mailed back.",
    },
    {
        "question": "What should I include when returning an item to Cymbal Bikes?",
        "answer": "Please include the original receipt and a copy of your shipping confirmation when returning an item.",
    },
    {
        "question": "Does Cymbal Bikes offer refunds for shipping charges?",
        "answer": "Cymbal Bikes does not offer refunds for shipping charges, except for defective items.",
    },
    {
        "question": "How do I process a return for a defective item at Cymbal Bikes?",
        "answer": "To process a return for a defective item, please contact Cymbal Bikes first.",
    },
]

"""#### Text

First we will try, `modality='text'`.
"""

question_set[0]

response = await rag(
    question=question_set[0]["question"],
    vector_db=vector_db_mini_vertex,
    embedding_client=client,  # For embedding generation
    embedding_model=text_embedding_model,  # For embedding model
    llm_client=client,  # For answer generation,
    top_k=3,
    llm_model=MODEL,
    modality="text",
)
display(Markdown(response))

"""#### Audio

Now, let's try `modality='audio'` to get audio response.
"""

await rag(
    question=question_set[-2]["question"],
    vector_db=vector_db_mini_vertex,
    embedding_client=client,  # For embedding generation
    embedding_model=text_embedding_model,  # For embedding model
    llm_client=client,  # For answer generation,
    top_k=3,
    llm_model=MODEL,
    modality="audio",
)

"""Evaluating Retrieval Augmented Generation (RAG) applications before production is crucial for identifying areas for improvement and ensuring optimal performance.
Check out the Vertex AI [Gen AI evaluation service](https://cloud.google.com/vertex-ai/generative-ai/docs/models/evaluation-overview).

## Conclusion

Congratulations on making it through this notebook!

- We have seen how to use the Gemini API in Vertex AI to generate text and Multimodal Live API to generate text and audio output.
-  Developed a fully functional Retrieval Augmented Generation (RAG) pipeline capable of answering questions based on provided documents.
- Demonstrated the versatility of the RAG architecture by enabling both text and audio output modalities.
- Ensured the adaptability of the RAG pipeline to various use cases by enabling seamless integration of different context documents.
- Established a foundation for building more advanced RAG systems leveraging larger document sets and sophisticated indexing/retrieval services like Vertex AI Datastore/Agent Builder and Vertex AI Multimodal Live API.

## What's next

- Learn how to [build a web application that enables you to use your voice and camera to talk to Gemini 2.0 through the Multimodal Live API.](https://github.com/GoogleCloudPlatform/generative-ai/tree/main/gemini/multimodal-live-api/websocket-demo-app)
- See the [Multimodal Live API reference docs](https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/multimodal-live).
- See the [Google Gen AI SDK reference docs](https://googleapis.github.io/python-genai/).
- Explore other notebooks in the [Google Cloud Generative AI GitHub repository](https://github.com/GoogleCloudPlatform/generative-ai).
"""